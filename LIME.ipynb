{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5a0236bf-6dbe-4ca7-94c5-fd09c78b30ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def kernel(X, Y, sigma=0.25):\n",
    "    d = np.zeros((X.shape[0], Y.shape[0]))\n",
    "    for i in range(X.shape[0]):\n",
    "        for j in range(Y.shape[0]):\n",
    "            d[i, j] = np.linalg.norm(X[i] - Y[j])\n",
    "    return np.exp(-(d ** 2) / sigma ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4fd0477f-bac5-40a3-9cc2-a58717ff51d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.segmentation import slic\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "#np.set_printoptions(threshold=sys.max size)\n",
    "import copy\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def create_perturbed_image_and_weights(img, kernel_size=4, max_dist=200, ratio = 0.25, num_samples=1000, kernel_width = 0.25):\n",
    "    permuted_img = img.permute(1,2,0)\n",
    "    segments = slic(permuted_img, n_segments=100, compactness=20, start_label=0)\n",
    "\n",
    "    n_features = segments.max() + 1\n",
    "    z_prime = np.random.randint(0, 2, num_samples*n_features).reshape((num_samples, n_features))\n",
    "    z_prime[0, :] = 1\n",
    "    grey_color = (torch.tensor([0.485, 0.456, 0.406]) * 255).numpy().astype(np.uint8)\n",
    "    grey_image = np.full((224, 224, 3), grey_color, dtype=np.uint8)\n",
    "    \n",
    "    #create z with z_prime\n",
    "    perturbed_images = []\n",
    "    for space in tqdm(z_prime):\n",
    "        temp = copy.deepcopy(permuted_img)\n",
    "        zeros = np.where(space == 0)[0]\n",
    "        mask = np.zeros(segments.shape).astype(bool)\n",
    "        for z in zeros:\n",
    "            mask[segments == z] = True\n",
    "            temp[mask] = torch.from_numpy(grey_image).float()[mask]\n",
    "        perturbed_images.append(temp.permute(2,0,1))\n",
    "        \n",
    "    perturbed_images = torch.stack(perturbed_images)\n",
    "    weights = kernel(z_prime[0].reshape(1, -1), z_prime, kernel_width)\n",
    "    return (segments, z_prime, weights, perturbed_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "663d9c97-f9ca-4f53-b21c-310a8dde8f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels_of_perturbed_images(perturbed_images, model, num_samples = 1000):\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        perturbed_images = perturbed_images.to('cuda')\n",
    "    with torch.no_grad():\n",
    "        perturbed_outputs = model(perturbed_images)  # Pass the batch through the model\n",
    "    return perturbed_outputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "49f2e42c-70fa-4624-8be8-6a323a33fa1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from skimage.color import label2rgb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def LIME_explanation(z_prime, perturbed_images_labels, target_label_idx, weights, segments, original_image, num_top_features=10):\n",
    "   \n",
    "    weights = np.asarray(weights).flatten()\n",
    "\n",
    "    target_label_column = perturbed_images_labels[:, target_label_idx]\n",
    "    \n",
    "    reg = Ridge(alpha=2.0, fit_intercept=True)\n",
    "    reg.fit(z_prime, target_label_column, sample_weight=weights)\n",
    "    \n",
    "    coefficients = reg.coef_\n",
    "    \n",
    "    top_features = np.argsort(-np.abs(coefficients))[:num_top_features]\n",
    "    \n",
    "    mask = np.zeros(segments.shape, dtype=bool)\n",
    "    for feature in top_features:\n",
    "        mask[segments == feature] = True\n",
    "    \n",
    "    original_image_np = original_image.permute(1, 2, 0).cpu().numpy()\n",
    "    highlighted_image = original_image_np.copy()\n",
    "    \n",
    "    dimmed_image = np.mean(original_image_np, axis=-1, keepdims=True) * 0.5  \n",
    "    highlighted_image[~ mask] = dimmed_image[~ mask] \n",
    "    \n",
    "    # Plot the result\n",
    "    plt.imshow(highlighted_image)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Highlighted Explanation with Original Colors\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95910201-4853-4095-bf57-6d8f719cad78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label: n03250847 (drumstick)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b669075fb0304bfa94c8cf776c89d508",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "import json\n",
    "import os \n",
    "\n",
    "# Load the pre-trained ResNet18 model\n",
    "model = models.resnet18(pretrained=True)\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "# Define the image preprocessing transformations\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406], \n",
    "        std=[0.229, 0.224, 0.225]   \n",
    "    )\n",
    "])\n",
    "\n",
    "# Load the ImageNet class index mapping\n",
    "with open(\"imagenet_class_index.json\") as f:\n",
    "    class_idx = json.load(f)\n",
    "idx2label = [class_idx[str(k)][1] for k in range(len(class_idx))]\n",
    "idx2synset = [class_idx[str(k)][0] for k in range(len(class_idx))]\n",
    "id2label = {v[0]: v[1] for v in class_idx.values()}\n",
    "\n",
    "idx2label_explanation = [class_idx[str(k)][1] for k in range(len(class_idx))]\n",
    "idx2synset_explanation = [class_idx[str(k)][0] for k in range(len(class_idx))]\n",
    "id2label_explanation = {v[0]: v[1] for v in class_idx.values()}\n",
    "\n",
    "imagenet_path = './imagenet_samples'\n",
    "\n",
    "# List of image file paths\\\n",
    "image_paths = [f for f in os.listdir(imagenet_path) if not f.startswith('.')]\n",
    "#image_paths = [image_paths[4]]\n",
    "for img_path in image_paths:\n",
    "    # Open and preprocess the image\n",
    "    my_img = os.path.join(imagenet_path, img_path)\n",
    "    input_image = Image.open(my_img).convert('RGB')\n",
    "    input_tensor = preprocess(input_image)\n",
    "    input_batch = input_tensor.unsqueeze(0)  # Create a mini-batch as expected by the model\n",
    "\n",
    "    # Move the input and model to GPU if available\n",
    "    if torch.cuda.is_available():\n",
    "        input_batch = input_batch.to('cuda')\n",
    "        model.to('cuda')\n",
    "\n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        output = model(input_batch)\n",
    "    # Get the predicted class index\n",
    "    _, predicted_idx = torch.max(output, 1)\n",
    "    predicted_idx = predicted_idx.item()\n",
    "    predicted_synset = idx2synset[predicted_idx]\n",
    "    predicted_label = idx2label[predicted_idx]\n",
    "    \n",
    "    predicted_class = torch.argmax(output, dim=1).item()\n",
    "    print(f\"Predicted label: {predicted_synset} ({predicted_label})\")\n",
    "\n",
    "    segments, z_prime, weights, perturbed_images = create_perturbed_image_and_weights(input_tensor.squeeze(), num_samples = 500)\n",
    "    \n",
    "    perturbed_outputs = labels_of_perturbed_images(perturbed_images, model)\n",
    "    perturbed_outputs_np = torch.softmax(perturbed_outputs, dim=1).cpu().numpy()\n",
    "    \n",
    "    target_label_idx = predicted_class\n",
    "\n",
    "    LIME_explanation(\n",
    "        z_prime=z_prime,\n",
    "        perturbed_images_labels=perturbed_outputs_np,\n",
    "        target_label_idx=target_label_idx,\n",
    "        weights=weights,\n",
    "        segments=segments,\n",
    "        original_image=input_tensor.squeeze(),\n",
    "        num_top_features=30\n",
    "    )\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7e8c9c-dea5-4612-b063-33303b3e8da0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
